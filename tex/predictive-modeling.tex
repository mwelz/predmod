\documentclass[12pt]{article}

\usepackage{hyperref,
		    xcolor, 
		    fancyhdr, 
		    graphicx,
		    caption,
		    subcaption,
		    float,
		    amsmath,
		    amssymb,
		    bm}

			
%%%%%%%%%%%%% LAYOUT %%%%%%	
\usepackage[
  top=2cm,
  bottom=2cm,
  left=1.5in,
  right=1.5in,
  headheight=17pt, % as per the warning by fancyhdr
  includehead,includefoot,
  heightrounded, % to avoid spurious underfull messages
]{geometry} 


\pagestyle{fancy}
\fancyhf{} 
\renewcommand{\headrulewidth}{1pt}
\fancyfoot[C]{\thepage}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{Performance Measures}

%%%%%%%% COLORS %%%
\hypersetup{
	hidelinks,
    colorlinks = true,
    linkbordercolor = {white},
    citecolor = blue,
    urlcolor = blue,
    linkcolor = blue,
    linktoc = all % make both sections and subsections in ToC clickable
}

% Figure caption options
\captionsetup{labelfont = {bf, sc, color=blue}} % figure label 
\captionsetup{font = {singlespacing, small}} % caption

% name of ToC
\renewcommand{\contentsname}{Table of Contents}


%%%%%%%%%% BIBLIOGRAPHY %%%%
\usepackage{natbib}
\bibliographystyle{apalike}

\begin{filecontents}{\jobname.bib}
@article{steyerberg2010,
  title={{Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures}},
  author={Steyerberg, Ewout W and Vickers, Andrew J and Cook, Nancy R and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J and Kattan, Michael W},
  journal={Epidemiology},
  volume={21},
  pages={128--138},
  year={2010},
  publisher={NIH Public Access}
}

\end{filecontents}

%%%%%%%%%%%% DOCUMENT %%%%%%%

\begin{document}
\section{Risk Modeling}
Let there be $n$ i.i.d. samples indexed $i=1,\dots,n$ of the shape $(Y_i, \mathbf{X}_i, W_i)$. The variable $Y_i$ denotes the outcome of interest, which is binary here (equals 1 in case of the individual's death), $\mathbf{X}_i$ denotes a $p$-vector of covariates, and $W_i$ is a binary treatment assignment indicator which takes the value one if the individual is in the treatment group.

In a first stage, we estimate the baseline mortality risk \textit{without} the inclusion of treatment assignment $W_i$. Concretely, we are interested in estimating $(p+1)$ coefficients captured in a $(p+1)$ vector $\bm{\theta}$ (the ``plus one'' stems from an intercept) by means of a linear mapping $g: \mathbb{R} \to (0,1)$:

\begin{equation} \label{eq:rm-stage1}
	\mathbb{P}(Y_i=1\ |\ \mathbf{X}_i = \mathbf{x}_i)
	=
	g \Big( \bm{\theta}^\top \mathbf{x}_i^* \Big),	
\end{equation}
for $i=1,\dots,n$, where the $(p+1)$-vector $\mathbf{x}_i^* = (1, \mathbf{x}_i^\top)^\top$ was defined to accommodate an intercept term. Note that if we choose $g$ to be the logistic function, equation \eqref{eq:rm-stage1} corresponds to a logistic regression model.

Suppose now that we have used equation \eqref{eq:rm-stage1} to estimate $\bm{\hat{\theta}}$ for a given choice of $g$. Then, given $\mathbf{X}_i = \mathbf{x}_i$, define the linear predictor $\widehat{lp}_i = \bm{\hat{\theta}}^\top \mathbf{x}_i^*$ and fix another linear mapping $h: \mathbb{R} \to (0,1)$. Then, in a second stage, estimate the two coefficients $\bm{\gamma} = (\gamma_0, \gamma_1)^\top$ in the model

\begin{equation}
	\mathbb{P} \Big( Y_i = 1\ |\ \mathbf{X}_i = \mathbf{x}_i, W_i = w_i, \widehat{lp}_i \Big) 
	=
	h\Big( 
	\widehat{lp}_i + \gamma_0 w_i + \gamma_1 (w_i \cdot \widehat{lp}_i)
	\Big),
\end{equation}
for $i=1,\dots,n$. This model estimates $\bm{\hat{\gamma}}$. Using this estimated coefficient vector $\bm{\hat{\gamma}}$, define $\widehat{\mathbb{P}}_i = h\Big( 
	\widehat{lp}_i + \hat{\gamma}_0 w_i + \hat{\gamma}_1 (w_i \cdot \widehat{lp}_i)
	\Big)$, given $W_i = w_i$.

Next, we then define a ``reversed'' treatment assignment $W_i^{rev}$ by  
\[
	W_i^{rev} = 
	\begin{cases}
	1 \text{ if } W_i=0,\\
	0 \text{ if } W_i=1.  
	\end{cases},
\]
and use it to calculate $\widehat{\mathbb{P}}_i^{rev} = h\Big( 
	\widehat{lp}_i + \hat{\gamma}_0 w_i^{rev} + \hat{\gamma}_1 (w_i^{rev} \cdot \widehat{lp}_i)
	\Big)$, given $W_i^{rev} = w_i^{rev}$.
	
We can then estimate the predicted benefit $pb_i$ by
\[
	\widehat{pb_i} = 
	\big(\widehat{\mathbb{P}}_i - 
	\widehat{\mathbb{P}}_i^{rev}
	\big) 
	\cdot
	(-1)^{w_i},
\]
given $W_i = w_i$, for $i=1,\dots,n$.

\newpage
This is based on \cite{steyerberg2010}. Let there be a given statistical prediction model. Measuring how close predictions are to the actual outcome can be done by measures such as explained variation ($R^2$). 


\section{Traditional Performance Measures}
Main focus is on distance between predicted outcome and actual outcome. For binary outcomes, $\hat{y} = p$, where $p$ is the predicted probability. 

See Table 1 in \cite{steyerberg2010} for a nice overview.

\subsection{Discrimination}
Idea: ``do patients with the outcome have higher risk predictions than those without?''

Accurate predictions discriminate between those with and those without the outcome. Several measures can be used to indicate how well we classify patients in a binary prediction problem. The concordance ($c$) statistic is the most commonly used performance measure to indicate the discriminative ability of generalized linear regression models. For a binary outcome, $c$ is identical to the area under the receiver operating characteristic (ROC) curve, which plots the sensitivity (true positive rate) against 1 â€“ specificity (false positive rate) for consecutive cut- offs for the probability of an outcome.

\subsection{Calibration}
Idea: ``do close to x of 100 patients
with a risk prediction of x\% have the outcome?''

Calibration refers to the agreement between observed outcomes and predictions. For example, if we predict a 20\% risk of residual tumor for a testicular cancer patient, the observed frequency of tumor should be approximately 20 of 100 patients with such a prediction. A graphical assessment of calibration is possible, with predictions on the x-axis and the outcome on the y-axis. Perfect predictions should be on the 45-degree line. For binary data, smoothing techniques can be used to estimate the observed probabilities of the outcome ($p(y=1)$) in relation to the predicted probabilities (like loess).




\bibliography{\jobname}
\end{document}

