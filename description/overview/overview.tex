\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, bm, amssymb, hyperref, xcolor, caption, dsfont}

% link coloring
\hypersetup{
	hidelinks,
    colorlinks = true,
    linkbordercolor = {white},
    citecolor = blue,
    urlcolor = blue,
    linkcolor = blue,
    linktoc = all 
}

% layout
\usepackage[
  top=2cm,
  bottom=2cm,
  left=1.5in,
  right=1.5in,
  headheight=17pt, % as per the warning by fancyhdr
  includehead,includefoot,
  heightrounded, % to avoid spurious underfull messages
]{geometry} 

% Figure caption options
\captionsetup{labelfont = {bf, sc, color=blue}} % figure label 
\captionsetup{font = {singlespacing, small}} % caption

%%  bib settings
\usepackage{natbib}
\setcitestyle{authoryear} 
\bibliographystyle{apalike} 

% author information
\title{\textsc{Predictive Modeling for Heterogeneous Treatment Effects in Medicine}}
\author{Max Welz \\
  \href{mailto:welz@ese.eur.nl}{\texttt{welz@ese.eur.nl}}}
  
\date{%
    \textsc{Econometric Institute\\ Erasmus School of Economics}\\[2ex]%
    \today}
    
% definitions
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\e}{\text{e}}
\newcommand{\indic}{\mathds{1}}


\begin{document}

\maketitle

\section{Introduction and Notation}

\subsection{Introduction}
The identification of heterogeneity in the effectiveness in medical trials has been traditionally done by \textit{one-variable-at-a-time-analyses}, where heterogeneity is evaluated along one single variable. For a number of statistical reasons, such analyses are insufficient for a reliable identification of treatment effect heterogeneity. Therefore, \cite{kent2020path} recommend to use predictive modeling approaches as well as machine learning techniques for this task. In this document, we introduce the proposed techniques.

\subsection{Notation}
For the remainder of this paper, let $\{(X_i, Y_i,W_i)\}_{i=1}^n$ be a random sample with the following characteristics. For $i = 1,\dots,n$, we assume that $X_i$ is a $p$-dimensional random vector of explanatory variables, $Y_i$ is a binary response variable, and $W_i$ is a binary treatment assignment variable. We say that individual $i$ \textit{has the event} if $Y_i=1$ and that $i$ has been treated if $W_i = 1$. We are interested in the causal effect of $W_i$ on $Y_i$ and potential heterogeneity therein. Using the well-known Rubin causal model, which defined the potential outcomes be defined as $Y_i(1)$ and $Y_i(0)$, denoting the outcome if individual $i$ is treated and if it is not treated, respectively. Recall that we only ever observe one of the potential outcomes for a given $i$, that is, $Y_i$ = $Y_i(W_i)$. The conditional average treatment effect of individual $i$ at an arbitrary covariate realization $X_i = x$ is formally defined as
\begin{equation}\label{eq:absolute-benefit}
    x \mapsto \theta(x) = \E[Y_i(1) - Y_i(0)\ |\ X_i = x].
\end{equation}
We are interested in estimating the treatment effect $x \mapsto \theta(x)$ on at least a group level, based on the observations $\{(X_i, Y_i,W_i)\}_{i=1}^n$. We frequently call $x \mapsto \theta(x)$ the \textit{absolute benefit} (of the treatment). Another quantity of interest is
\begin{equation}\label{eq:relative-benefit}
    x \mapsto \theta^{rel}(x) = \frac{\E[Y_i(1) | X_i = x]}{\E[Y_i(0) | X_i = x]},
\end{equation}
which is called the \textit{relative benefit} (of the treatment). At $X_i = x_i$, we often write $\theta_i = \theta(x_i)$ and $\theta^{rel}_i = \theta^{rel}(x_i)$.

\section{One-Variable-at-a-Time Analyses}
One-variable-at-a-time analyses are typically by means of a series of exact rate ratio tests. Suppose the $n$ samples at hand can be partitioned into two disjoint groups, $\mathcal{G}_0$ and $\mathcal{G}_1$, and these groups are not necessarily collectively exhaustive. The  partitioning is usually done according to treatment assignment along some characteristics of a single variable in $X_i$, hence the name \textit{one-variable-at-a-time analysis}. For example, $\mathcal{G}_0$ and $\mathcal{G}_1$ may contain the younger-than-60-years-old individuals in the control group and treatment group, respectively. 

The random variables 
\[
    P_0 = |\{i \in \mathcal{G}_0 : Y_i = 1\}|
    \quad \text{ and } \quad
    P_1 = |\{i \in \mathcal{G}_1 : Y_i = 1\}|
\]
count the number of events in each group. We furthermore assume that we observe the time at risk of of each individual $i$, denoted $T_i$. Let
\[
    N_0 = \sum_{\{i:i\in\mathcal{G}_0\}} T_i
    \quad \text{ and } \quad
    N_1 = \sum_{\{i:i\in\mathcal{G}_1\}} T_i
\]
denote the cumulative time at risk within each group. We assume that the random variables $P_0,P_1$ that measure number of events in each group are Poisson-distributed as
\[
    P_0\sim \textsf{Poisson}(N_0 \lambda_0)
    \quad \text{ and } \quad
    P_1\sim \textsf{Poisson}(N_1 \lambda_1),
\]
where the parameters $\lambda_0, \lambda_1 >0$ are unknown. This assumption implies that we can interpret the cumulative number of life years in each group, $N_0$ and $N_1$, as the total time spent in the Poisson processes $P_0$ and $P_1$, respectively. We are interested in the \textit{rate ratio} $\theta$, defined by
\[
    \theta = \frac{\lambda_0}{\lambda_1}.
\]
Typically, the uniformly most powerful (UMP) test (e.g. p. 152 in \citealp{lehmann1986}) is used for testing the rate ratio $\theta$. See the vignette of the \href{https://cran.r-project.org/web/packages/rateratio.test/index.html}{\texttt{rateratio.test} package} on the CRAN for details on this test. If we reject the null hypothesis $H_0: \theta \neq 1$, we have found evidence that there seem to be systematic differences in the occurrence of events between the two groups. For instance, if $Y_i=1$ denotes that individual $i$ has died and $T_i$ measures the number life years of $i$, this test can be used to test for systematic differences in mortality rates between the treatment and control group. If a significant difference is found, this difference is due to the treatment intervention (that is at least the idea).

\section{Predictive Models}
In the following, we introduce the techniques proposed in \cite{kent2020path}. This section is partially based on \cite{rekkas2019}. Note that neither \cite{kent2020path} nor \cite{rekkas2019} propose their methods in an unambiguous way. Hence, this section is an attempt at a mathematically rigorous and statistically sound definition of predictive models. Predictive models can be broken down in \textit{risk models} and \textit{effect models}. The goal of any predictive model is to estimate the absolute and relative benefit in equations \eqref{eq:absolute-benefit} and \eqref{eq:relative-benefit}, respectively.

\subsection{Risk Models}\label{sec:risk-model}
The idea behind risk models is to separately estimate the effect of $X_i$ on $Y_i$ and the effect of $W_i$ on $Y_i$. In doing so, one separates the explanatory power for $Y_i$ into a part that is due to $X_i$ and a part that is due to $W_i$. This gives rise to a two-stage estimation procedure, which is known as risk modeling.

\subsubsection{Stage 1: Baseline Risk} \label{sec:baseline-risk}
In stage one, we fit a linear logistic model of $X_i$ to $Y_i$, which assumes the linear identity
\begin{equation} \label{eq:baseline-identity}
    \ln \left( \frac{ \P[Y_i=1 | X_i = x_i] }{1 - \P[Y_i=1 | X_i = x_i]} \right)
    =
    \beta_0 + x_i^\top \beta, 
\end{equation}
for all $i=1,\dots,n$, where $(\beta_0, \beta) = (\beta_0,\beta_1,\dots,\beta_p)\in\R^{p+1}$ is some fixed vector of coefficients. Fitting this model by means of cross-validated regularized logistic regression (see Appendix \ref{sec:logistic-regression} for details) yields estimates $(\widehat{\beta_0}, \widehat{\beta})$. Due to the regularization penalty in the corresponding optimization problem, the estimate will be sparse; the idea behind this is to separate more relevant predictors from less relevant ones.  
The estimates $(\widehat{\beta_0}, \widehat{\beta})$ can then be used to compute an estimate of the \textit{baseline risk}, $\widehat{\P}[Y_i=1 | X_i = x_i]$, as well as an estimate of the \textit{linear predictor}, $\widehat{\eta}_i$, which corresponds to the estimated right-hand side of~\eqref{eq:baseline-identity} at $X_i = x_i$:
\begin{equation*}
    \widehat{\eta}_i = \widehat{\eta}_i(x_i) = \widehat{\beta}_0 + x_i^\top \widehat{\beta}.
\end{equation*}
We retain the linear predictors $\widehat{\eta}_i$ for the second stage.

\subsubsection{Stage 2: Final Risk}
In stage two, we fit another linear logistic model, this time of $W_i$ and the interaction $W_i \widehat{\eta}_i$ to $Y_i$, where $\widehat{\eta}_i$ is the linear predictor from Stage 1. We moreover assume an offset of the value $\widehat{\eta}_i$. Formally, the assumed model satisfies 
\begin{equation} \label{eq:stage2-model}
    \ln \left( 
    \frac{ \P[Y_i=1 | W_i = w_i, \eta_i = \widehat{\eta}_i] }{1 - \P[Y_i=1 | W_i = w_i, \eta_i = \widehat{\eta}_i]} \right)
    =
    \alpha_0 + \alpha_1 w_i + \alpha_2 w_i \widehat{\eta}_i + \widehat{\eta}_i 
\end{equation}
for all $i=1,\dots,n$, where $\alpha_0, \alpha_1, \alpha_2 \in \R$ are coefficients. We fit this model by means of non-regularized logistic regression. The obtained estimates of the fitted model, $(\widehat{\alpha}_0, \widehat{\alpha}_1, \widehat{\alpha}_2)$, can then be used to calculate the \textit{risk prediction function} for $w\in\{0,1\}$:
\begin{equation} \label{eq:rm-risk}
\begin{split}
    \textsf{risk}_i(w)
    &:=
    \widehat{\P}[Y_i=1 | W_i = w, \eta_i = \widehat{\eta}_i]\\
    &=
    \Big( 1  +  \exp\big( -\widehat{\alpha}_0 - \widehat{\alpha}_1w - \widehat{\alpha}_2 w\widehat{\eta}_i - \widehat{\eta}_i \big)\Big)^{-1}\\
    &= F_{logistic}\Big(\widehat{\alpha}_0 + \widehat{\alpha}_1w + \widehat{\alpha}_2 w\widehat{\eta}_i + \widehat{\eta}_i\Big),
\end{split}
\end{equation}
where $F_{logistic}$ is the distribution function of the logistic distribution with location zero and scale one. 

\subsection{Effect Models} \label{sec:effect-model}
Effect models attempt to model treatment effect heterogeneity explicitly by adding interaction terms. The idea is that (regularized) logistic regression shrinks the coefficients of interaction terms (and other variables) with low explanatory variables to zero, resulting in a parsimonious model for treatment effect heterogeneity.

However, if one wants to perform inference in a model in which variable selection took place, one needs to address the additional uncertainty stemming from the selection process. This issue is addressed in \cite{wasserman2009high}. We correspondingly adapt the model selection of an effect model as proposed in \cite{kent2020path} by using the strategy suggested in \cite{wasserman2009high}. The following steps explain the ensuing effect model selection strategy.

\paragraph{Step 0. Initialization.} Fix some $\alpha \in [0,1]$. Let $\mathcal{I}\subset [p] := \{1,\dots,p\}$ be the set of variables that are to be interacted with the treatment assignment variable.\footnote{The choice $\mathcal{I}=\varnothing$ is also permitted (i.e. no variable is interacted), but we decidedly do not reflect this possibility in our notation for the sake of notational brevity.} Let $\D_1,\D_2,\D_3$ be approximately equally sized, mutually disjoint subsets of the sample space, that is, $\bigcup_{i=1}^3 \D_i = [n]$, while $\bigcap_{i=1}^3 \D_i = \varnothing$. Let $\Lambda_n$ be a finite set of pre-specified choices of the regularization tuning parameter $\lambda$. The parameter choices in $\Lambda_n$ constitute the grid along which we perform cross-validation.\footnote{See \cite{friedman2010} for details on the construction of the grid $\Lambda_n$.} 

\paragraph{Step 1. Fitting of a single effect model.} Fix some $\lambda \in \Lambda_n$. For all $i = 1,\dots, n$, we consider the linear logistic model identified by
\begin{equation} \label{eq:effect-model-individual}
    \ln \left( \frac{ \P[Y_i = 1 \ |\ W_i = w_i, X_i = x_i] }{1 - \P[Y_i = 1 \ |\ W_i = w_i, X_i = x_i]} \right)
    =
    \beta_0 + x_i^\top \beta + \gamma_0w_i + \sum_{\{j:j\in\mathcal{I}\}} \gamma_j w_i x_{ij},
\end{equation}
where $x_{ij}$ is the $j$-th element in vector $x_i$, whereas $\beta_0, \gamma_0\in\R, \beta\in\R^p$, and $\gamma := \{ \gamma_j \}_{j\in\mathcal{I}} \in \R^{|\mathcal{I}|}$ are coefficients. Collect the non-intercept coefficients in a vector of dimension $q = 1 + p + |\mathcal{I}|$, denoted $\vartheta := (\vartheta_1,\dots,\vartheta_q) := (\beta, \gamma_0, \gamma)$. The $q$-dimensional vector of independent variables that is associated with the coefficient vector $\vartheta$ is given by $z_i := (z_{i1},\dots,z_{iq}) := \big(x_i, w_i, \{ w_i x_{ij} \}_{j\in\mathcal{I}}\big)$.

Define by $\mathcal{L}_{\mathcal{D}}(\beta_0, \vartheta)$ the log-likelihood function\footnote{This log-likelihood function is given by $\mathcal{L}_{\mathcal{D}}(\beta_0, \vartheta) = \sum_{\{ i:i \in \D \}} \Big( Y_i \big(\beta_0 + z_i^\top \vartheta \big) - \ln\big( 1 + \e^{\beta_0 + z_i^\top \vartheta} \big)\Big)$.} corresponding to model \eqref{eq:effect-model-individual}, evaluated at some sample subset $\mathcal{D}\subset [n]$. We fit the model by solving
\begin{equation} \label{eq:effect-lambda-problem}
    (\widetilde{\beta}_{0,\lambda}, \widetilde{\vartheta}_\lambda)
    =
    \arg\min_{(\beta_0, \vartheta) \in \R^{q+1}}
    \Bigg\{
    - \frac{1}{|\D_1|} \mathcal{L}_{\D_1}(\beta_0, \vartheta) 
    + 
    \lambda \Big( 
    (1-\alpha) \| \vartheta \|_2^2 + \alpha \|\vartheta \|_1
    \Big)
    \Bigg\},
\end{equation}
that is, we obtain coefficient estimates $(\widetilde{\beta}_{0,\lambda}, \widetilde{\vartheta}_\lambda)$ by only using information from the observations in set $\D_1$. The estimates $(\widetilde{\beta}_{0,\lambda}, \widetilde{\vartheta}_\lambda)$ depend on the choice of tuning parameter $\lambda$; we make this dependence explicit in our notation by including a $\lambda$-subscript. Next, collect the retained variables in a suite $\widehat{S}_n(\lambda) := \left\{ j \in [q]: \widetilde{\vartheta}_{j,\lambda} \neq 0 \right\}$. Denote by $\widehat{L}(\lambda) := - \mathcal{L}_{\D_2}(\widetilde{\beta}_{0,\lambda}, \widetilde{\vartheta}_\lambda)$ the empirical loss associated with tuning parameter $\lambda$. The empirical loss here corresponds to value of the negative log-likelihood evaluated at the observations in $\D_2$ and the the coefficient estimates $(\widetilde{\beta}_{0,\lambda}, \widetilde{\vartheta}_\lambda)$. Fitting the model \eqref{eq:effect-lambda-problem} and evaluating  its empirical loss on two distinct, separate sample subsets helps to prevent overfitting \citep{wasserman2009high}.

\paragraph{Step 2. Cross-validation.} The optimal choice for the tuning parameter $\lambda$ in \eqref{eq:effect-lambda-problem} corresponds to the choice that minimizes the empirical loss, that is,
\[
    \widehat{\lambda} = \arg \min_{\lambda\in\Lambda_n} \widehat{L}(\lambda).
\]
The \textit{reduced model}, denoted $\widehat{S}_n$, contains the retained variables of the model associated with the optimal tuning parameter $\lambda$:
\[
    \widehat{S}_n := \widehat{S}_n(\widehat{\lambda}) \subset [q].
\]

\paragraph{Step 3. Inference and final model.} For $i\in [n]$, recall that $z_i = (z_{i1},\dots, z_{ij}) = \big(x_i, w_i, \{ w_i x_{ij} \}_{j\in\mathcal{I}}\big)$ is the $q$-dimensional vector that holds the independent variables in the linear logistic model \eqref{eq:effect-model-individual} and that $\vartheta = (\vartheta_1, \dots, \vartheta_q)$ is the associated vector of coefficients. Potentially, some of the coefficients in $\vartheta$ have been shrunk to zero in the reduced model $\widehat{S}_n$. Using a slight abuse of set theoretic notation, denote by
\begin{equation}\label{eq:reduced-vectors}
    z_{S,i}
    =
    \Big\{
        z_{ij} \in z_i : j \in \widehat{S}_n
    \Big\}
    \quad \text{ and } \quad
    \vartheta_{S} 
    = 
    \Big\{
        \vartheta_j \in \vartheta : j \in \widehat{S}_n
    \Big\}
\end{equation}
the vector of independent variables that are retained in the reduced  model $\widehat{S}_n$ and its associated coefficient vector, respectively. Both vectors are of dimension $m := |\widehat{S}_n|$, which corresponds to the number of retained variables in the reduced model.

On $\widehat{S}_n$ (that is, with these reduced vectors), we can subsequently perform logistic regression by using only the observations in the third sample subset, $\D_3$. Concretely, we minimize the negative log-likelihood 
\begin{equation*} 
    \big(\widehat{\beta}_0, \widehat{\vartheta}_S\big)
    =
    \arg\min_{(\beta_0, \vartheta_S) \in \R^{m+1}}
    \Bigg\{
    - 
    \sum_{\{ i:i \in \D_3 \}} \bigg( Y_i \big(\beta_0 + z_{S,i}^\top \vartheta_S \big) - \ln\Big( 1 + \exp \big(\beta_0 + z_{S,i}^\top \vartheta_S\big) \Big)\bigg)\Bigg\}.
\end{equation*}

We can now construct the final model by performing inference. Elementary logistic regression theory allows us to compute a covariance matrix estimate $\widehat{\V}\big(\widehat{\vartheta}_{\widehat{S}_n}\big)$ with which we can compute $t$-test statistics $T_j = \widehat{\vartheta}_{S, j} \Big/ \widehat{\V}\big(\widehat{\vartheta}_{S,j}\big)$ for all $j \in \widehat{S}_n$. Thereupon we construct the \textit{final model} by
\[
    \widehat{D}_n = \Big\{ j\in\widehat{S}_n : |T_j| > c_n \Big\},
\]
where $c_n = z_{\delta/2m}$ is the $(\delta/2m)$-th upper quantile of the standard normal distribution for pre-specified significance level $\delta\in(0,0.5)$. Using the notation in \eqref{eq:reduced-vectors}, the final model's  estimated coefficient vector and vector of independent variables are given by  $\widehat{\vartheta}_D$ and $z_{D,i}$, respectively. Both vectors are of length $d := |\widehat{D}_n|$.

\paragraph{Step 4. Risk prediction with the final model.}
Given some $w \in \{0,1\}$, the \textit{risk prediction function} of the (final) effect model is, for all $i=1,\dots,n$, given by
\begin{equation}\label{eq:em-risk}
    \textsf{risk}_i(w)
    :=
    \widehat{\P}[Y_i=1 | W_i = w, X_i = x_i] 
    =
    F_{logistic}\Big(\widehat{\beta}_0 +
    z_{D,i}^\top \widehat{\vartheta}_{D}\Big), 
\end{equation}
where $z_{D,i}$ depends on $w$: recall that the original vector is given by $z_i = (z_{i1},\dots, z_{ij}) = \big(x_i, w_i, \{ w_i x_{ij} \}_{j\in\mathcal{I}}\big)$, which can be expressed as $z_i = \big(x_i, w, \{ w x_{ij} \}_{j\in\mathcal{I}}\big)$ to make the dependence on $w$ explicit.

Observe that it may happen that treatment assignment $W_i = w_i$ is not among the retained variables in the final model $\widehat{D}_n$. This is problematic because if the treatment assignment is not retained, then the risk prediction function $w \mapsto \textsf{risk}_i(w)$ in \eqref{eq:em-risk} is not defined. \cite{kent2020path} do not address this possibility. To overcome this potential issue, we could artificially add the treatment assignment to the retained variables. That is, if $j_w\in[q]$ is the index of the treatment assignment variable, we would define the reduced model and final model by
\[
    \widehat{S}_n = \{j_w\} \cup \widehat{S}_n(\widehat{\lambda})
    \quad \text{ and } \quad
    \widehat{D}_n =  \{j_w\} \cup \Big\{ j\in\widehat{S}_n : |T_j| > c_n \Big\},
\]
respectively. However, this solution is potentially problematic from a statistical point of view \textcolor{red}{[Add some explanations here. @Andreas: do you agree that this solution is problematic? If yes, a potential alternative could be to to keep the coefficient of the treatment assignment, $\gamma_0$, out of the regularization penalty in \eqref{eq:effect-lambda-problem} to avoid that it gets shrunk to zero. What do you think about this potential solution?]}

\subsection{Estimation of Benefits}
Suppose we have fitted a predictive model, either a risk model (as in Section \ref{sec:risk-model} or an effect model (as in Section \ref{sec:effect-model}). Consider the risk prediction function $w \mapsto \textsf{risk}_i(w), i=1,\dots,n$, associated with the fitted model. 
For the observed treatment status of individual $i$, $W_i = w_i$, we define the \textit{regular risk} as $\textsf{risk}_i^{\textsf{reg}} = \textsf{risk}_i(w_i)$. Furthermore, define the \textit{reversed} treatment assignment by
\[
    W_i^{rev} = 
    \begin{cases}
    1 \qquad \text{ if } W_i = 0,\\
    0 \qquad \text{ if } W_i = 1.
    \end{cases}
\]
Thereupon, at $W_i^{rev} = w_i^{rev}$, we define the \textit{reverse risk} by $\textsf{risk}_i^{\textsf{rev}} = \textsf{risk}_i(w_i^{rev})$. We estimate the absolute benefit in \eqref{eq:absolute-benefit} by the \textit{predicted absolute benefit}, $\widehat{\theta}_i$,
\[
    \widehat{\theta}_i 
    =
    \textsf{risk}_i
    -
    \textsf{risk}_i^{\textsf{rev}}
\]
and the relative benefit in \eqref{eq:relative-benefit} by the \textit{predicted relative benefit}, $\widehat{\theta}^{rel}_i$, 
\[
    \widehat{\theta}_i^{rel} 
    =
    \frac{\textsf{risk}_i}{\textsf{risk}_i^{\textsf{rev}}}.
\]

Effectively, estimation of the predictive benefits consists of estimating the potential outcomes. However, \cite{chernozhukov2020generic} show that such approaches only give noisy (and therefore inconsistent) estimates of the causal parameter of interest. We use the predictive benefits to measure the strength of treatment effect heterogeneity, which will be discussed in the next section.

\subsection{Measuring Heterogeneity}
Suppose we have obtained predictive absolute and relative benefits, $\widehat{\theta}_i$ and $\widehat{\theta}^{rel}_i$, respectively, for $i=1,\dots,n$. Suppose furthermore that the observations $\{1,\dots,n\}$ can be grouped into $m$ groups which we index by $1,2,\dots,m$. Denote the group membership of observation $i$ by $G_i \in \{1,\dots,m\} = \mathcal{G}$. The grouping is usually done based on characteristics in the covariates $X_i$. Denote by 
\begin{equation} \label{eq:group-level-benefits}
\begin{split}
    \widehat{\theta}_g 
    &=
    |\{i\in [n]:G_i = g\}|^{-1}
    \sum_{\{i\in [n]:G_i = g\}}
    \widehat{\theta}_i \quad \text{ and }
    \\
    \widehat{\theta}^{rel}_g 
    &=
    |\{i\in [n]:G_i = g\}|^{-1}
    \sum_{\{i\in [n]:G_i = g\}}
    \widehat{\theta}_i^{rel}
\end{split}
\end{equation}
the within-group estimates, for groups $g\in\mathcal{G}$. Since the within-group estimates are just means, one can use elementary $t$-tests for testing for differences between group estimates, which is intended as inference for the strength of treatment effect heterogeneity along various groups. 

\subsection{Measuring Calibration}
In order to assess whether a fitted predictive model provides an accurate fit to the data, calibration plots are used. For this purpose, let the grouping in the previous section be done based on quantiles of the estimated baseline risk of having the event (see Section~\ref{sec:baseline-risk}). Consider these groups as fixed for the remainder of this section. Moreover, calculate the group-level predicted benefits as in \eqref{eq:group-level-benefits} on all groups $g\in\mathcal{G}$. A \textit{calibration plot} plots the group-level predicted benefits against the \textit{group-level observed benefits}. If the the group-level predicted benefits and group-level observed benefits roughly lie on a 45 degrees line, we say that the predictive model is well calibrated. In the following, we show how group-level observed benefits can be obtained.

\subsubsection{Absolute Observed Benefit}
For each group $g \in \mathcal{G}$ calculate the group-level \textit{absolute} observed benefit $\widehat{aob}_g$ by
\begin{equation}
\begin{split}
    \widehat{aob}_g = &|\{ i : G_i = g, W_i = 1\}|^{-1} \sum_{\{ i : G_i = g, W_i = 1 \}} Y_i
    - \\
    &|\{ i : G_i = g, W_i = 0\}|^{-1} \sum_{\{ i : G_i = g, W_i = 0 \}} Y_i.
\end{split}
\label{eq:aob-def}
\end{equation}
Since the group-level absolute observed benefit is essentially the difference in the average group-level mortality, we can view it as an estimate of the group-level average treatment effect.

Due to this ``difference in means'' structure, we can use elementary statistical theory to construct confidence intervals for the absolute observed benefit $\widehat{aob}_g$. Concretely, we can construct a \textit{Welch t-test} for performing tests on $\widehat{aob}_g$. We use a Welch $t$-test rather than a classical $t$-test for the sake of generality: Unlike a classical $t$-test, Welch's test does not assume equal population variances of the two summands of $\widehat{aob}_g$ in equation \eqref{eq:aob-def}.

\subsubsection{Relative Observed Benefit}
For each group $g \in \mathcal{G}$ calculate the group-level \textit{relative} observed benefit $\widehat{rob}_g$ by the relative risk (or risk ratio)
\begin{equation}
    \widehat{rob}_g = \frac{|\{ i : G_i = g, W_i = 1\}|^{-1} \sum_{\{ i : G_i = g, W_i = 1 \}} Y_i
    }{|\{ i : G_i = g, W_i = 0\}|^{-1} \sum_{\{ i : G_i = g, W_i = 0 \}} Y_i}.
\label{eq:rob-def}
\end{equation}
Observe that the relative observed benefit corresponds to the ratio of the two summands that constitute the absolute observed benefit in equation \eqref{eq:aob-def} and can be viewed as a ratio of two proportions. The relative observed benefit estimates the theoretical group-level risk ratio
\begin{equation}
    rr_g = \frac{\mathbb{P}(Y_1 = 1 | W_1 = 1, G_1 = g)}{\mathbb{P}(Y_1 = 1 | W_1 = 0, G_1 = g)}.
\label{eq:rr_g}
\end{equation}

Constructing a confidence interval for the group-level relative observed benefit $\widehat{rob}_g$ is somewhat more involved than for the absolute observed benefit, because the ratio $\widehat{rob}_g$ usually does not asymptotically follow a normal distribution. Using the procedure suggested \href{https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/bs704_confidence_intervals8.html}{\texttt{here}}, we construct confidence intervals for $\widehat{rob}_g$ by using a two-step procedure.

We first need some ancillary quantities. Within a given group $g \in \mathcal{G}$ let the constants
\begin{equation}
\begin{split}
    N_g^{(Y=1, W=1)} &= |\{i : G_i = g, Y_i = 1, W_i = 1\}|,
    \\
    N_g^{(Y=0, W=1)} &= |\{i : G_i = g, Y_i = 0, W_i = 1\}|,
    \\
    N_g^{(Y=1, W=0)} &= |\{i : G_i = g, Y_i = 1, W_i = 0\}|, \text{ and }
    \\
    N_g^{(Y=0, W=0)} &= |\{i : G_i = g, Y_i = 0, W_i = 0\}|
\end{split}
\end{equation}
count the number of treated deaths, treated survivors, untreated deaths, and untreated survivors, respectively.\footnote{Usually these counters are represented in a $2 \times 2$ contingency table as in \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/}{\texttt{here}}.} Similarly, let
\begin{equation}
    N_g^{(W=1)} = |\{i : G_i = g, W_i = 1\}|
    \quad \text{and} \quad 
    N_g^{(W=0)} = |\{i : G_i = g, W_i = 0\}|
\end{equation}
count the number of treated and untreated samples, respectively, in group $g$. For a given group $g \in \mathcal{G}$, denote by 
\begin{equation}
    Z_g 
    =
    \frac{N_g^{(Y=0, W=1)}}{N_g^{(Y=1, W=1)}}
    \bigg/ N_g^{(W=1)}
    +
    \frac{N_g^{(Y=0, W=0)}}{N_g^{(Y=1, W=0)}}
    \bigg/ N_g^{(W=0)}
\end{equation}
the sum of the relative survival for the treatment and control group (normalized by the number of samples in each of these groups). 

Finally, for a given group $g \in \mathcal{G}$, a corresponding relative observed benefit $\widehat{rob}_g$, a significance level $\alpha \in (0,0.5)$, and a $(1-\alpha)$-quantile of the standard normal distribution $z_{1-\alpha / 2}$, denote the real-valued interval $\mathcal{C}^{\log(rr)}_g$ by 
\begin{equation}
    \mathcal{C}^{\log(rr)}_g
    =
    \Big[
    \log \big( \widehat{rob}_g\big) - z_{1-\alpha / 2} \sqrt{Z_g},\ 
    \log \big( \widehat{rob}_g\big) + z_{1-\alpha / 2} \sqrt{Z_g}
    \Big].
\label{eq:ci-log(rr)}
\end{equation}
The interval $\mathcal{C}'_g$ is a $(1-\alpha)$-confidence interval for $\log(rr_g)$. A simple exponential transformation of \eqref{eq:ci-log(rr)} yields the desired $(1-\alpha)$-confidence interval for $rr_g$:
\begin{equation}
\begin{split}
    \mathcal{C}_g^{rr}
    &=
    \Bigg[ 
    \exp\Big( \log \big( \widehat{rob}_g\big) - z_{1-\alpha / 2} \sqrt{Z_g} \Big),
    \exp\Big( \log \big( \widehat{rob}_g\big) + z_{1-\alpha / 2} \sqrt{Z_g} \Big)
    \Bigg]
    \\
    &=
   \Bigg[ 
   \widehat{rob}_g 
   \exp \Big(
   - z_{1-\alpha / 2} \sqrt{Z_g} \Big),\ 
   \widehat{rob}_g 
   \exp \Big(
    z_{1-\alpha / 2} \sqrt{Z_g} \Big)
    \Bigg].
\end{split}
\end{equation}

\subsection{Empirical Odds Ratio}
The empirical odds ratio is often used as an additional performance measure and is discussed here for the sake of completeness. 
Define for the relevant group-level theoretical probabilities $p_g^{(1)} = \mathbb{P}(Y_1 = 1 | W_1 = 1, G_1 = g)$ and $p_g^{(0)} = \mathbb{P}(Y_1 = 1 | W_1 = 0, G_1 = g)$. Then, the relative risk in \eqref{eq:rr_g} can be written as $rr_g = p_g^{(1)} / p_g^{(0)}$. The theoretical group-level \textit{odds ratio} is defined as
\begin{equation}
    or_g = \frac{p_g^{(1)} / (1 - p_g^{(1)})}{p_g^{(0)} / (1 - p_g^{(0)})}.
\end{equation}

Define now the empirical probabilities by 
\begin{align*}
\hat{p}_g^{(1)} &= \frac{N_g^{(Y=1,W=1)}}{N_g^{(Y=1,W=1)} + N_g^{(Y=0,W=1)}} \quad \text{and}
\\
\hat{p}_g^{(0)} &= \frac{N_g^{(Y=1,W=0)}}{N_g^{(Y=1,W=0)} + N_g^{(Y=0,W=0)}}.
\end{align*}
Analogously to the theoretical group-level odds ratio $or_g$, the \textit{empirical} odds ratio $\widehat{or}_g$ is given by
\begin{equation}
    \widehat{or}_g = \frac{\hat{p}_g^{(1)} / (1 - \hat{p}_g^{(1)})}{\hat{p}_g^{(0)} / (1 - \hat{p}_g^{(0)})}.
\end{equation}

Similarly to the relative observed benefit, we cannot immediately construct a confidence interval for the theoretical group-level odds ratio $or_g$, but need a two-step procedure.
The variance of the estimator $\widehat{or}_g$ is given by
\begin{equation}
    V_g = \frac{1}{N_g^{(Y=1, W=1)}} + 
    \frac{1}{N_g^{(Y=0, W=1)}} +
    \frac{1}{N_g^{(Y=1, W=0)}} + 
    \frac{1}{N_g^{(Y=0, W=0)}}.
\end{equation}
For a given significance level $\alpha \in (0,0.5)$ and a $(1-\alpha)$-quantile of the standard normal distribution $z_{1-\alpha / 2}$, denote the real-valued interval $\mathcal{C}_g^{\log (or)}$ by
\begin{equation}
    \mathcal{C}^{\log(or)}_g
    =
    \Big[
    \log \big( \widehat{or}_g\big) - z_{1-\alpha / 2} \sqrt{V_g},\ 
    \log \big( \widehat{or}_g\big) + z_{1-\alpha / 2} \sqrt{V_g}
    \Big].
\end{equation}
The interval $\mathcal{C}_g^{\log (or)}$ is a $(1-\alpha)$-confidence interval for $\log(or_g)$. A simple exponential transformation yields he desired $(1-\alpha)$-confidence interval for $or_g$:
\begin{equation}
\begin{split}
    \mathcal{C}_g^{or}
    &=
    \Bigg[ 
    \exp\Big( \log \big( \widehat{or}_g\big) - z_{1-\alpha / 2} \sqrt{V_g} \Big),
    \exp\Big( \log \big( \widehat{or}_g\big) + z_{1-\alpha / 2} \sqrt{V_g} \Big)
    \Bigg]
    \\
    &=
   \Bigg[ 
   \widehat{or}_g 
   \exp \Big(
   - z_{1-\alpha / 2} \sqrt{V_g} \Big),\ 
   \widehat{or}_g 
   \exp \Big(
    z_{1-\alpha / 2} \sqrt{V_g} \Big)
    \Bigg].
\end{split}
\end{equation}



\bibliography{bibliography}
\newpage
\appendix
\section*{Appendix}


\section{(Regularized) Logistic Regression}\label{sec:logistic-regression}
Assume we are interested in predicting binary outcomes $Y_i$ via $q$-dimensional random vectors $Z_i$, where $\{(Z_i, Y_i)\}_{i=1}^n$ is a random sample. Hence, since $Y_i|Z_i \sim \textsf{Bernoulli}(p_{i,Z})$, we are effectively interested in estimating the parameter $p_{i,Z} = \P[Y_i = 1 \ |\ Z_i]$, where the subscript ``$Z$" reminds us that $p_{i,Z}$ is a conditional probability.

We assume a logistic linear model, which assumes the linear identity
\begin{equation}\label{eq:logistic-identity}
    \ln \left( \frac{p_{i,Z}}{1 - p_{i,Z}} \right)
    =
    \beta_0 + Z_i^\top \beta, 
\end{equation}
for all $i=1,\dots,n$, where $(\beta_0, \beta) = (\beta_0,\beta_1,\dots,\beta_q)\in\R^{q+1}$ is some fixed vector of coefficients. Rearranging identifies the probability of interest, $p_{i,Z}$, as
\begin{equation*}
    p_{i,Z} = \left( 1 + \exp\left( -\beta_0 - Z_i^\top \beta\right) \right)^{-1}.
\end{equation*}
We can fit the model \eqref{eq:logistic-identity} by \textit{logistic regression}, which performs maximum likelihood estimation over the coefficients. The corresponding optimization problem can be shown to solve
\begin{equation}\label{eq:nonpenalized-likelohood}
    (\widehat{\beta_0}, \widehat{\beta})
    \in 
    \arg \min_{(\beta_0, \beta) \in \R^{p+1}}
    \left\{
    -
    \frac{1}{n} \sum_{i=1}^n \left(
    Y_i (\beta_0 + Z_i^\top \beta)
    -
    \ln 
    \left(
    1 + \e^{\beta_0 + Z_i^\top \beta}
    \right)
    \right)\right\}.
\end{equation}

However, when there are more explanatory variables than observations, $q \geq n$, then the solution $(\widehat{\beta_0}, \widehat{\beta})$ may be degenerate. In this case, adding a regularization penalty to the objective function in  \eqref{eq:nonpenalized-likelohood} enforces sparsity in the solution and thereby renders the solution stable again. Concretely, given fixed $\lambda_n \geq 0$ and $\alpha \in [0,1]$, \textit{regularized logistic regression} solves
\begin{equation}\label{eq:penalized-likelohood}
\begin{split}
    (\widehat{\beta_0}, \widehat{\beta})
    \in 
    \arg \min_{(\beta_0, \beta) \in \R^{p+1}}
    \Bigg\{
    -
    \frac{1}{n} \sum_{i=1}^n \left(
    Y_i (\beta_0 + Z_i^\top \beta)
    -
    \ln 
    \left(
    1 + \e^{\beta_0 + Z_i^\top \beta}
    \right)
    \right) 
    +
    \\
    +
    \lambda_n \Big( 
    (1-\alpha) \| \beta \|_2^2 + \alpha \|\beta \|_1
    \Big)
    \Bigg\}.
\end{split}
\end{equation}
Observe that for $\alpha=0.5$, we obtain the elastic net penalty and for $\alpha=1$, we obtain the Lasso penalty. We can find an appropriate choice of $\lambda_n$ by cross-validation over a grid of candidate values for $\lambda_n$. See \cite{friedman2010} for numerical details. 

Suppose we have obtained an estimate $(\widehat{\beta}_0, \widehat{\beta})$ of $(\beta_0, \beta)$ in the identity \eqref{eq:logistic-identity}, either by regularized or non-regularized logistic regression. Then we can estimate the probabilities of interest $\P[Y_i =1\ |\ X_i]$ by
\[
    \widehat{p}_{i,Z} = \left( 1 + \exp\left( -\widehat{\beta}_0 - Z_i^\top \widehat{\beta}\right) \right)^{-1}.
\]



% nonrandom treatment assignment

\end{document}
